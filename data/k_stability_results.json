{
  "k_results": [
    {
      "k": 1,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.5,
      "auroc_std": 0.0
    },
    {
      "k": 2,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.65,
      "auroc_std": 0.056
    },
    {
      "k": 3,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.68,
      "auroc_std": 0.045
    },
    {
      "k": 4,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.721,
      "auroc_std": 0.041
    },
    {
      "k": 5,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.728,
      "auroc_std": 0.035
    },
    {
      "k": 6,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.754,
      "auroc_std": 0.03
    },
    {
      "k": 7,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.771,
      "auroc_std": 0.024
    },
    {
      "k": 8,
      "n": 50,
      "accuracy": 0.52,
      "auroc_sigma2_answer_CORRECT": 0.775,
      "auroc_std": 0.0
    }
  ],
  "bug_explanation": "The original k_stability_analysis.py (swift-cedar session) used a BUGGY sigma2_answer = variance of binary correct/incorrect labels (gold-dependent, anti-calibrated AUROC=0.12-0.50). Corrected metric: pairwise answer token disagreement (gold-free, AUROC=0.500->0.775 for K=1->8).",
  "correct_metric_description": "sigma2_answer = 1 - mean_pairwise_agreement(token pairs)",
  "analysis_script": "experiments/k_stability_reanalysis.py"
}