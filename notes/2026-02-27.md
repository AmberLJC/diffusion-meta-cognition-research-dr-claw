# Research Notes â€” 2026-02-27

_Session: 12:11 AM UTC | First deep research run_

---

## ðŸ”¬ State of the Art: Diffusion Language Models (Feb 2026)

### Key Models Active in Literature
- **LLaDA** (Large Language Diffusion with mAsking): Masked diffusion LLM, 8B params. Base model for much downstream work.
- **Dream-7B**: Another masked diffusion LLM used as benchmark comparison.
- **MDLM** (Masked Diffusion Language Model): Earlier framework, now scaled (see "Scaling Beyond MDLM", Feb 2026).
- **SEDD**: Score Entropy Discrete Diffusion, continuous-time formulation.
- **VDLM** (Variable Diffusion LMs, Jan 2026): Latent-to-text rendering, addresses fixed-length constraint.
- **LLaDA-V** (May 2025): Multimodal extension of LLaDA.
- **LLaDA-MoE** (Sept 2025): Sparse mixture-of-experts variant.
- **LLaDA 1.5** (May 2025): Variance-reduced preference optimization for alignment.

### Recent Key Papers (Feb 2026)

#### 1. DLM-Scope: Mechanistic Interpretability of Diffusion LMs via Sparse Autoencoders
- **arXiv:2602.05859** | Feb 5, 2026
- First SAE-based interpretability framework for DLMs
- **Critical finding**: Inserting SAEs into DLMs *reduces* cross-entropy loss in early layers (opposite to AR LLMs!)
- SAE features provide useful signals for **decoding order** in DLMs
- SAE features are stable during post-training
- Opens: probing for knowledge, calibration, meta-cognitive signals via SAE features

#### 2. TDGNet: Hallucination Detection in Diffusion LMs via Temporal Dynamic Graphs
- **arXiv:2602.08048** | Feb 8, 2026
- Formulates hallucination detection as learning over evolving token-level attention graphs
- Sparsifies attention graph at each denoising step; applies temporal attention to aggregate trajectory-wide evidence
- Tested on LLaDA-8B and Dream-7B, QA benchmarks
- **Key insight**: Factuality evidence is *distributed across the denoising trajectory* and may drift, appear, or self-correct over time
- Outperforms single-pass, output-based, and static-graph baselines

#### 3. "Reasoning in Diffusion LLMs Concentrated in Dynamic Confusion Zones"
- **arXiv:2511.15208** | Nov 19, 2025
- Introduces **Confusion Zones**: transient spikes in uncertainty during denoising trajectory
- Metrics: entropy-based uncertainty, Confidence-Margin (CM), Rate of Entropy Change (RoEC)
- **Most steps are stable; confusion zones are sparse but highly predictive of final success/failure**
- Proposes ATPO (Adaptive Trajectory Policy Optimization): focus RL gradient on confusion zone steps
- **This is the closest paper to our meta-cognition direction â€” but never connects to knowledge boundary or calibration**

#### 4. MAGE: All-[MASK] Block Already Knows Where to Look
- **arXiv: (Feb 15, 2026)**
- In fully masked state (t=T), diffusion LLM attention already encodes "where to look" for decoding
- This implies **proto-planning even before any tokens are revealed** â€” a form of implicit global intent

#### 5. Prism: Test-Time Scaling via Hierarchical Search + Self-Verification for Discrete Diffusion LMs
- Feb 2, 2026
- First TTS algorithm adapted for non-AR diffusion decoding
- Uses hierarchical search + self-verification (model verifies its own outputs mid-trajectory)
- **Self-verification is a form of meta-cognition!** â€” but paper treats it as a decoding strategy, not an epistemic study

#### 6. Search or Accelerate: Confidence-Switched Beam Search for Diffusion LMs
- **arXiv: (Feb 25, 2026)**
- Switches between beam search (for uncertain tokens) and greedy (for confident tokens)
- Uses per-token confidence to decide decoding strategy
- **Operationalizes token-level confidence in DLMs** but doesn't study calibration or knowledge boundaries

#### 7. Stopping Computation for Converged Tokens (Feb 6, 2026)
- Tokens that have "converged" during denoising get no further compute
- Defines convergence via stability across steps â€” another form of implicit confidence

### The Interpretability Gap
The **mechanistic interpretability** search for "diffusion language model mechanistic interpretability" returns almost nothing beyond DLM-Scope. AR LLMs have 100s of papers on circuits, attention heads, probing classifiers, logit lens, etc. DLMs have ~1 paper (DLM-Scope) applying SAEs.

---

## ðŸ§  Meta-Cognition in LLMs: Where AR Research Stands

AR LLM meta-cognition research (2024-2025):
- **Probing classifiers** for factual recall confidence
- **Verbalized uncertainty** (model says "I'm not sure")
- **Conformal prediction** for calibrated set generation
- **Semantic entropy** (Kuhn et al.) â€” cluster-based uncertainty from multiple AR samples
- **P(IK)** (predict "I know") â€” AR training signal for self-knowledge
- **Logit lens / tuned lens** â€” reading hidden states at intermediate layers as early predictions

**None of these have been applied to diffusion LMs.** The diffusion setting fundamentally changes the setup:
- No left-to-right token probability sequence
- Uncertainty lives in the denoising trajectory, not in a next-token distribution
- Models can *revise* tokens (unlike AR)
- Global bidirectional context from step 0

---

## ðŸ”­ Gap Analysis: What Nobody Has Done

| Research Area | AR LLMs | Diffusion LLMs |
|---|---|---|
| Mechanistic interpretability (SAEs) | Mature | Just started (DLM-Scope, Feb 2026) |
| Calibration / uncertainty quantification | Mature | **Nothing** |
| Self-knowledge / "I know vs I don't know" | Several papers | **Nothing** |
| Knowledge boundary detection | Moderate | **Nothing** |
| Meta-cognitive token analysis | Emerging | **Nothing** |
| Hallucination detection | Very mature | TDGNet (Feb 2026) â€” trajectory-based |
| Confusion zone â†’ epistemic calibration | N/A | **Unexplored** |
| Probing for factual recall confidence | Several | **Nothing** |
| Uncertainty geometry across denoising | N/A | **Nothing** |

---

## ðŸ’¡ Novel Direction Identified: "Confusion-Zone Calibration"

**Central insight**: The "Dynamic Confusion Zones" paper shows that **uncertainty spikes during denoising strongly predict final success/failure** at the *reasoning task* level. But nobody has asked:

> Does the *geometry* of confusion zones during denoising encode **epistemic uncertainty** â€” i.e., whether the model *knows the facts* or not?

This is fundamentally different from TDGNet (which detects hallucinations post-hoc) and from ATPO (which uses confusion zones for RL training). Our direction:

**Confusion-Zone Epistemic Calibration (CZEC)**:
1. Run a DLM on factual QA (TriviaQA, NQ, PopQA)
2. Extract confusion zone features: entropy curve shape, RoEC peaks, CM trajectory, total confusion mass, confusion zone count
3. Correlate these features with factual correctness
4. Ask: can we build a **zero-shot calibration signal** from confusion zone geometry alone?
5. Compare to AR baselines: semantic entropy, verbalized confidence, token probability entropy
6. Sub-question: Do **more difficult / less known facts** produce qualitatively different confusion zone signatures?

**Why this is novel**:
- Combines confusion zone dynamics (known phenomenon) + meta-cognitive calibration (unexplored for DLMs)
- Zero-shot: no fine-tuning needed, purely from trajectory analysis
- Potentially more powerful than AR calibration because DLMs have full bidirectional context
- Bridges mechanistic interpretability (DLM-Scope SAE features) with uncertainty quantification

**Why nobody has done it**:
- TDGNet focuses on hallucination *detection* via attention graphs, not calibration via entropy trajectory
- ATPO uses confusion zones for *training* not *inference-time self-assessment*
- No AR-style calibration work has migrated to DLMs

---

## ðŸ§ª Proposed Experiments

### Experiment 1: Confusion Zone â†’ Factual Confidence (Pilot)
- Model: LLaDA-8B (public weights)
- Dataset: TriviaQA (500-1000 questions)
- Metrics: entropy at each step, RoEC, CM
- Extract: confusion zone count, total confusion mass, peak RoEC, CM profile
- Label: is final answer correct?
- Analysis: AUROC of confusion zone features as hallucination/uncertainty predictors
- Baseline: AR model (GPT-2 or LLaMA) semantic entropy

### Experiment 2: Knowledge Boundary Segmentation
- Same setup but stratify by difficulty (PopQA by entity popularity)
- Hypothesis: "known" facts have fewer/smaller confusion zones
- "Unknown/hallucinated" facts have higher and more sustained confusion

### Experiment 3: SAE + Confusion Zone Joint Signal
- Use DLM-Scope-style SAE features at confusion zone steps
- Do specific SAE features activate at confusion zones for known vs unknown facts?
- This could identify *knowledge boundary circuits* in DLMs

---

## ðŸ“Ž Papers to Track

- arXiv:2602.05859 â€” DLM-Scope (mechanistic interp, SAEs)
- arXiv:2602.08048 â€” TDGNet (hallucination via temporal attention)
- arXiv:2511.15208 â€” Confusion Zones / ATPO (trajectory uncertainty)
- LLaDA original paper (masked diffusion LLM)
- arXiv:2602.17XXX â€” MAGE (all-mask attention)
- arXiv:2602.05XXX â€” Prism (TTS for diffusion)
