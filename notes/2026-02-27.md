# Research Notes â€” 2026-02-27

_Session: 12:11 AM UTC | First deep research run_

---

## ğŸ”¬ State of the Art: Diffusion Language Models (Feb 2026)

### Key Models Active in Literature
- **LLaDA** (Large Language Diffusion with mAsking): Masked diffusion LLM, 8B params. Base model for much downstream work.
- **Dream-7B**: Another masked diffusion LLM used as benchmark comparison.
- **MDLM** (Masked Diffusion Language Model): Earlier framework, now scaled (see "Scaling Beyond MDLM", Feb 2026).
- **SEDD**: Score Entropy Discrete Diffusion, continuous-time formulation.
- **VDLM** (Variable Diffusion LMs, Jan 2026): Latent-to-text rendering, addresses fixed-length constraint.
- **LLaDA-V** (May 2025): Multimodal extension of LLaDA.
- **LLaDA-MoE** (Sept 2025): Sparse mixture-of-experts variant.
- **LLaDA 1.5** (May 2025): Variance-reduced preference optimization for alignment.

### Recent Key Papers (Feb 2026)

#### 1. DLM-Scope: Mechanistic Interpretability of Diffusion LMs via Sparse Autoencoders
- **arXiv:2602.05859** | Feb 5, 2026
- First SAE-based interpretability framework for DLMs
- **Critical finding**: Inserting SAEs into DLMs *reduces* cross-entropy loss in early layers (opposite to AR LLMs!)
- SAE features provide useful signals for **decoding order** in DLMs
- SAE features are stable during post-training
- Opens: probing for knowledge, calibration, meta-cognitive signals via SAE features

#### 2. TDGNet: Hallucination Detection in Diffusion LMs via Temporal Dynamic Graphs
- **arXiv:2602.08048** | Feb 8, 2026
- Formulates hallucination detection as learning over evolving token-level attention graphs
- Sparsifies attention graph at each denoising step; applies temporal attention to aggregate trajectory-wide evidence
- Tested on LLaDA-8B and Dream-7B, QA benchmarks
- **Key insight**: Factuality evidence is *distributed across the denoising trajectory* and may drift, appear, or self-correct over time
- Outperforms single-pass, output-based, and static-graph baselines

#### 3. "Reasoning in Diffusion LLMs Concentrated in Dynamic Confusion Zones"
- **arXiv:2511.15208** | Nov 19, 2025
- Introduces **Confusion Zones**: transient spikes in uncertainty during denoising trajectory
- Metrics: entropy-based uncertainty, Confidence-Margin (CM), Rate of Entropy Change (RoEC)
- **Most steps are stable; confusion zones are sparse but highly predictive of final success/failure**
- Proposes ATPO (Adaptive Trajectory Policy Optimization): focus RL gradient on confusion zone steps
- **This is the closest paper to our meta-cognition direction â€” but never connects to knowledge boundary or calibration**

#### 4. MAGE: All-[MASK] Block Already Knows Where to Look
- **arXiv: (Feb 15, 2026)**
- In fully masked state (t=T), diffusion LLM attention already encodes "where to look" for decoding
- This implies **proto-planning even before any tokens are revealed** â€” a form of implicit global intent

#### 5. Prism: Test-Time Scaling via Hierarchical Search + Self-Verification for Discrete Diffusion LMs
- Feb 2, 2026
- First TTS algorithm adapted for non-AR diffusion decoding
- Uses hierarchical search + self-verification (model verifies its own outputs mid-trajectory)
- **Self-verification is a form of meta-cognition!** â€” but paper treats it as a decoding strategy, not an epistemic study

#### 6. Search or Accelerate: Confidence-Switched Beam Search for Diffusion LMs
- **arXiv: (Feb 25, 2026)**
- Switches between beam search (for uncertain tokens) and greedy (for confident tokens)
- Uses per-token confidence to decide decoding strategy
- **Operationalizes token-level confidence in DLMs** but doesn't study calibration or knowledge boundaries

#### 7. Stopping Computation for Converged Tokens (Feb 6, 2026)
- Tokens that have "converged" during denoising get no further compute
- Defines convergence via stability across steps â€” another form of implicit confidence

### The Interpretability Gap
The **mechanistic interpretability** search for "diffusion language model mechanistic interpretability" returns almost nothing beyond DLM-Scope. AR LLMs have 100s of papers on circuits, attention heads, probing classifiers, logit lens, etc. DLMs have ~1 paper (DLM-Scope) applying SAEs.

---

## ğŸ§  Meta-Cognition in LLMs: Where AR Research Stands

AR LLM meta-cognition research (2024-2025):
- **Probing classifiers** for factual recall confidence
- **Verbalized uncertainty** (model says "I'm not sure")
- **Conformal prediction** for calibrated set generation
- **Semantic entropy** (Kuhn et al.) â€” cluster-based uncertainty from multiple AR samples
- **P(IK)** (predict "I know") â€” AR training signal for self-knowledge
- **Logit lens / tuned lens** â€” reading hidden states at intermediate layers as early predictions

**None of these have been applied to diffusion LMs.** The diffusion setting fundamentally changes the setup:
- No left-to-right token probability sequence
- Uncertainty lives in the denoising trajectory, not in a next-token distribution
- Models can *revise* tokens (unlike AR)
- Global bidirectional context from step 0

---

## ğŸ”­ Gap Analysis: What Nobody Has Done

| Research Area | AR LLMs | Diffusion LLMs |
|---|---|---|
| Mechanistic interpretability (SAEs) | Mature | Just started (DLM-Scope, Feb 2026) |
| Calibration / uncertainty quantification | Mature | **Nothing** |
| Self-knowledge / "I know vs I don't know" | Several papers | **Nothing** |
| Knowledge boundary detection | Moderate | **Nothing** |
| Meta-cognitive token analysis | Emerging | **Nothing** |
| Hallucination detection | Very mature | TDGNet (Feb 2026) â€” trajectory-based |
| Confusion zone â†’ epistemic calibration | N/A | **Unexplored** |
| Probing for factual recall confidence | Several | **Nothing** |
| Uncertainty geometry across denoising | N/A | **Nothing** |

---

## ğŸ’¡ Novel Direction Identified: "Confusion-Zone Calibration"

**Central insight**: The "Dynamic Confusion Zones" paper shows that **uncertainty spikes during denoising strongly predict final success/failure** at the *reasoning task* level. But nobody has asked:

> Does the *geometry* of confusion zones during denoising encode **epistemic uncertainty** â€” i.e., whether the model *knows the facts* or not?

This is fundamentally different from TDGNet (which detects hallucinations post-hoc) and from ATPO (which uses confusion zones for RL training). Our direction:

**Confusion-Zone Epistemic Calibration (CZEC)**:
1. Run a DLM on factual QA (TriviaQA, NQ, PopQA)
2. Extract confusion zone features: entropy curve shape, RoEC peaks, CM trajectory, total confusion mass, confusion zone count
3. Correlate these features with factual correctness
4. Ask: can we build a **zero-shot calibration signal** from confusion zone geometry alone?
5. Compare to AR baselines: semantic entropy, verbalized confidence, token probability entropy
6. Sub-question: Do **more difficult / less known facts** produce qualitatively different confusion zone signatures?

**Why this is novel**:
- Combines confusion zone dynamics (known phenomenon) + meta-cognitive calibration (unexplored for DLMs)
- Zero-shot: no fine-tuning needed, purely from trajectory analysis
- Potentially more powerful than AR calibration because DLMs have full bidirectional context
- Bridges mechanistic interpretability (DLM-Scope SAE features) with uncertainty quantification

**Why nobody has done it**:
- TDGNet focuses on hallucination *detection* via attention graphs, not calibration via entropy trajectory
- ATPO uses confusion zones for *training* not *inference-time self-assessment*
- No AR-style calibration work has migrated to DLMs

---

## ğŸ§ª Proposed Experiments

### Experiment 1: Confusion Zone â†’ Factual Confidence (Pilot)
- Model: LLaDA-8B (public weights)
- Dataset: TriviaQA (500-1000 questions)
- Metrics: entropy at each step, RoEC, CM
- Extract: confusion zone count, total confusion mass, peak RoEC, CM profile
- Label: is final answer correct?
- Analysis: AUROC of confusion zone features as hallucination/uncertainty predictors
- Baseline: AR model (GPT-2 or LLaMA) semantic entropy

### Experiment 2: Knowledge Boundary Segmentation
- Same setup but stratify by difficulty (PopQA by entity popularity)
- Hypothesis: "known" facts have fewer/smaller confusion zones
- "Unknown/hallucinated" facts have higher and more sustained confusion

### Experiment 3: SAE + Confusion Zone Joint Signal
- Use DLM-Scope-style SAE features at confusion zone steps
- Do specific SAE features activate at confusion zones for known vs unknown facts?
- This could identify *knowledge boundary circuits* in DLMs

---

## ğŸ“ Papers to Track

- arXiv:2602.05859 â€” DLM-Scope (mechanistic interp, SAEs)
- arXiv:2602.08048 â€” TDGNet (hallucination via temporal attention)
- arXiv:2511.15208 â€” Confusion Zones / ATPO (trajectory uncertainty)
- LLaDA original paper (masked diffusion LLM)
- arXiv:2602.17XXX â€” MAGE (all-mask attention)
- arXiv:2602.05XXX â€” Prism (TTS for diffusion)

---

# Research Update â€” 2026-02-27 12:26 AM UTC
_Session: Run #2 | Deep follow-up + new paper discovery_

---

## ğŸ†• Critical New Papers Found (Run #2)

### 1. "Your Absorbing Discrete Diffusion Secretly Models the Bayesian Posterior"
- **arXiv:2507.07586** | Cooper Doyle | Jul 2025
- **MAJOR THEORETICAL FINDING** â€” changes everything about our calibration direction
- **Proof**: Under mild assumptions, the denoiser of absorbing discrete diffusion (= masked diffusion LMs like LLaDA, MDLM) implements the **exact Bayesian posterior** over the original tokens
- **MC estimator**: K independent denoising passes â†’ posterior means AND variances; converges at O(1/âˆšK)
- **Key metric**: Per-token variance from K passes has **Spearman Ï = 0.996** with reconstruction error on WikiText-2
- **Limitation**: Only tested on WikiText-2 perplexity. Never applied to:
  - Factual QA (TriviaQA, NQ, PopQA)
  - Knowledge boundary detection
  - Calibrated confidence for factual recall
  - Comparison to AR semantic entropy
- **Gap this creates**: We have a theoretical proof that DLMs implement Bayesian posteriors. Nobody has used this to build a **factual calibration system** or study **knowledge boundaries**.

### 2. "The Energy of Falsehood: Detecting Hallucinations via Diffusion Model Likelihoods"
- **arXiv: (Feb 11, 2026)** | Submitted February 2026
- **Method**: "Generative Stress Test" â€” corrupt a claim with noise â†’ reconstruct using discrete text diffusion â†’ measure "Semantic Energy" (NLI-based divergence between original and reconstruction)
- **Hypothesis**: True facts lie on the generative manifold (reconstruct cleanly); hallucinations are unstable (don't reconstruct)
- **Key difference from CZEC/our direction**:
  - Energy of Falsehood: **External validation** â€” takes completed statements and stress-tests them
  - Our direction: **Internal generation** â€” studies trajectory dynamics WHILE the model is generating
  - Energy of Falsehood: Requires an NLI critic as external judge
  - Our direction: Purely trajectory-based, zero external components needed
  - Energy of Falsehood: Tests a claim's fidelity to the generative manifold
  - Our direction: Extracts epistemic uncertainty FROM THE GENERATION PROCESS ITSELF
- **Threat assessment update**: This paper partially occupies the "use diffusion for hallucination detection" space. But it's a reconstruction-based external checker, not trajectory-based internal calibration. Our direction is **complementary and more powerful** (applies during generation, not post-hoc).

### 3. "Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration"
- **arXiv: (Feb 9, 2026)**
- Uses diffusion IDEAS (noise injection, reconstruction) to improve calibration of REGULAR transformers
- **Not DLMs** â€” this is about using diffusion as a tool for AR transformers
- Does not study trajectory dynamics, confusion zones, or Bayesian posteriors
- Not a competitor to our direction

### 4. "Discrete Stochastic Localization for Non-autoregressive Generation"
- Feb 17, 2026
- Iterative refinement / error accumulation in masked diffusion
- Focuses on generation quality, not epistemic uncertainty

### 5. "dUltra: Ultra-Fast Diffusion LMs via RL" (Feb 2026)
- Speed-focused RL training for DLMs
- Not relevant to calibration/meta-cognition

---

## ğŸ§  Updated Gap Analysis (Post Run #2)

### The Doyle-Gap: Theory Exists, No Applications

Doyle (Jul 2025) proved DLMs implement Bayesian posteriors. **Nothing has been built on this proof for practical calibration**:

| What Doyle Showed | What Nobody Has Done |
|---|---|
| DLM denoiser = exact Bayesian posterior | Apply to factual QA calibration |
| K-pass MC estimator converges at O(1/âˆšK) | Test on knowledge recall (vs perplexity only) |
| Per-token variance Ï=0.996 with reconstruction error | Compare to AR semantic entropy on factual QA |
| WikiText-2 perplexity validation only | Knowledge boundary detection |
| Theoretical | Confusion zone connection |

### The Energy-of-Falsehood Differentiation

| Paper | Signal | Source | When | Requires |
|---|---|---|---|---|
| Energy of Falsehood | Reconstruction divergence | External stress test | Post-generation | NLI critic |
| Our direction (BPFQ) | Posterior variance / confusion geometry | Internal generation | During generation | Nothing extra |
| TDGNet | Temporal attention graphs | Internal | Post-generation | Attention extraction |
| CZEC (our original) | Confusion zone entropy trajectory | Internal | During generation | Trajectory logging |

---

## ğŸ’¡ REVISED #1 Direction: Bayesian Posterior Factual Calibration (BPFC)

**Upgraded central insight**: We now have BOTH:
1. A **theoretical foundation** (Doyle: DLMs implement Bayesian posteriors â†’ per-token variance is grounded uncertainty)
2. An **empirical phenomenon** (Chen et al.: Confusion zones predict success/failure)

These have never been connected to **factual calibration** for knowledge-intensive QA.

### The Core Research Question (Sharpened)
> Does the per-token variance from the Bayesian posterior MC estimator (K denoising passes over a masked diffusion LM) constitute a calibrated epistemic uncertainty signal for **factual knowledge recall** â€” and does it exhibit qualitatively different patterns for known facts vs. hallucinations?

### Why This Is Now Stronger Than Run #1

1. **Theoretical backing**: We're not just empirically testing a hunch â€” Doyle's proof gives us a principled reason to expect this to work
2. **Clear differentiation from Energy of Falsehood**: Internal trajectory analysis â‰  external reconstruction stress test
3. **Missing link**: Doyle's WikiText-2 perplexity â†’ knowledge QA is a concrete, publishable extension
4. **Unified framework**: Posterior variance (Doyle) + Confusion zones (Chen et al.) + Knowledge boundaries (our extension) = a trifecta nobody has assembled

### Proposed New Experiment Architecture

```
Input: Factual QA question + answer span (LLaDA-8B-Instruct)

Phase 1 â€” Posterior Variance Extraction:
  - Perform K=32 independent masked denoising passes
  - For each pass k, record final token predictions P_k(x_i) for each position i
  - Compute: ÏƒÂ²_i = Var_k[P_k(x_i)]  â† per-token posterior variance
  - Aggregate: ÏƒÂ²_total = mean(ÏƒÂ²_i over answer span positions)
  - This is the Bayesian-posterior-grounded uncertainty measure

Phase 2 â€” Confusion Zone Features:
  - For a single run, extract entropy trajectory H(t)
  - Extract: RoEC peaks, confusion mass, confusion count, peak step
  - These are the empirical confusion features

Phase 3 â€” Joint Analysis:
  - Correlate: ÏƒÂ²_total with factual correctness â†’ AUROC
  - Correlate: confusion_mass with ÏƒÂ²_total â†’ do they agree?
  - Correlate: ÏƒÂ²_total with entity frequency (knowledge boundary proxy)
  - Compare: ÏƒÂ²_total vs. AR semantic entropy (same questions, LLaMA-8B, 5 samples)
  - Build: ECE (Expected Calibration Error) curves for all methods

Phase 4 â€” Knowledge Boundary Analysis:
  - Stratify by PopQA entity frequency (1-star â†’ 5-star popularity)
  - Hypothesis: ÏƒÂ²_total monotonically decreases with entity frequency
  - Hypothesis: K=4 passes sufficient for high-frequency facts; K=32 needed for boundaries
```

### Predicted Novel Findings
1. **ÏƒÂ²_total AUROC â‰¥ 0.75** for factual correctness prediction (vs. 0.65 for single-pass token entropy)
2. **ÏƒÂ²_total and confusion_mass correlate** (r â‰¥ 0.7) â€” confirming two different views of the same phenomenon
3. **Knowledge boundary signature**: Known facts â†’ ÏƒÂ² < 0.01; Hallucinated answers â†’ ÏƒÂ² > 0.05
4. **DLM calibration â‰¥ AR baseline** (ÏƒÂ²-based ECE < semantic entropy ECE)
5. **K=8 is a "sweet spot"** â€” enough passes for stable posterior estimate, practical compute cost

### Connection to Downstream Applications
- **Selective generation**: Generate only when ÏƒÂ²_total < threshold
- **RAG integration**: Route to retrieval when ÏƒÂ²_total exceeds knowledge boundary threshold
- **Abstention**: DLM can say "I don't know" based purely on internal posterior variance
- **Factual editing**: Target positions with high ÏƒÂ² for knowledge editing (DLM-ROME equivalent)

---

## ğŸ”¬ New Direction #3 Candidate: Posterior Variance as Factual Editing Localization

**Novel idea** (not in any paper): Use per-position posterior variance ÏƒÂ²_i to localize WHERE in the model factual knowledge is stored â€” a DLM equivalent of ROME (Rank-One Model Editing).

In AR models (ROME), causality tracing is used to identify which MLP layer stores a fact. In DLMs:
- We don't have causal structure
- But we have bidirectional attention + Bayesian posterior variance per position
- **Hypothesis**: Positions where ÏƒÂ²_i is highest during factual recall are positions where the model is "drawing on" stored knowledge
- **Method**: Probe DLM-Scope SAEs at high-ÏƒÂ² positions for factual vs. counterfactual facts
- **Payoff**: First mechanistic localization of factual storage in a bidirectional model

---

## ğŸ“ Updated Reference List

| Paper | arXiv | Date | Status | Relevance |
|---|---|---|---|---|
| DLM-Scope (SAEs) | 2602.05859 | Feb 5, 2026 | Known | Foundation for probing |
| TDGNet (temporal attention) | 2602.08048 | Feb 8, 2026 | Known | Related competitor |
| Confusion Zones / ATPO | 2511.15208 | Nov 2025 | Known | Core building block |
| Bayesian Posterior in DLMs | 2507.07586 | Jul 2025 | NEW | Theoretical foundation for BPFC |
| Energy of Falsehood | ~2602.XXXXX | Feb 11, 2026 | NEW | Competitor (different method) |
| Diffusion-Insp. Calibration | ~2602.XXXXX | Feb 9, 2026 | NEW | Not directly competing |
| Search-or-Accelerate | (Feb 25, 2026) | Feb 2026 | Known | Token confidence baseline |
| Prism (TTS/self-verify) | (Feb 2, 2026) | Feb 2026 | Known | Self-verification precedent |
| MAGE (all-mask attention) | (Feb 15, 2026) | Feb 2026 | Known | Proto-planning structure |
| Doyle 2025 | 2507.07586 | Jul 2025 | NEW â†’ anchor | Posterior variance â†’ calibration |

